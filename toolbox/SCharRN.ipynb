{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "import sys\n",
    "# https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters 241906\n",
      "Vocab Size 126\n"
     ]
    }
   ],
   "source": [
    "filename = './corpus/gandhi/gandhi.txt'\n",
    "\n",
    "text = open(filename).read().lower()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# n_chars = len(text)\n",
    "n_chars = int(len(text) / 200)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print('Total Characters', n_chars)\n",
    "print('Vocab Size', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of 241856 99.7800% done'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare dataset of input to output pairs\n",
    "import pickle\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "save = True\n",
    "\n",
    "if save:\n",
    "    seq_length = 50\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "\n",
    "    iterations = n_chars - seq_length\n",
    "    \n",
    "    print(iterations)\n",
    "    \n",
    "    for i in range(0, iterations, 1):\n",
    "        display_text = 'of ' + str(iterations) + ' {:.4f}% done'.format(float(i) / float(iterations) * 100)\n",
    "        clear_output(wait=True)\n",
    "        display(display_text)\n",
    "        \n",
    "        seq_in = text[i:i + seq_length]\n",
    "        seq_out = text[i + seq_length]\n",
    "    #     seq_out = ' ' if seq_out == ' ' else '*'\n",
    "        dataX.append([char_to_int[char] for char in seq_in])\n",
    "        dataY.append(char_to_int[seq_out])\n",
    "\n",
    "    n_patterns = len(dataX)\n",
    "    print('Total Patterns', n_patterns)\n",
    "    pickle.dump((dataX, dataY), open('Xy.dat', 'wb'))\n",
    "else:\n",
    "    (dataX, dataY) = pickle.load(open('Xy.dat', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "X = X / float(vocab_size)\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model and loaded weights from file\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "load_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some callbacks\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "generate = PeakOnGeneration()\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "callbacks_list = [checkpoint, generate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "241856/241856 [==============================] - 148s 610us/step - loss: 2.8465\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.84651, saving model to weights-improvement-01-2.8465.hdf5\n",
      "Seed: your knowledge of the vegetarian literature will e\n",
      "o toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe to\n",
      "Epoch 2/20\n",
      "241856/241856 [==============================] - 151s 624us/step - loss: 2.7804\n",
      "\n",
      "Epoch 00002: loss improved from 2.84651 to 2.78042, saving model to weights-improvement-02-2.7804.hdf5\n",
      "Seed: worthy brother after\n",
      "money would be in my returnin\n",
      " toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe\n",
      "Epoch 3/20\n",
      "241856/241856 [==============================] - 151s 624us/step - loss: 2.7428\n",
      "\n",
      "Epoch 00003: loss improved from 2.78042 to 2.74283, saving model to weights-improvement-03-2.7428.hdf5\n",
      "Seed: indeed, some\n",
      "fashion as a goddess have discarded s\n",
      "o the toete to the toete to the toete to the toete to the toete to the toete to the toete to the toete to the toete to the toete to the toete to the toete to the toete to the toete to the toete to the\n",
      "Epoch 4/20\n",
      " 92900/241856 [==========>...................] - ETA: 1:30 - loss: 2.7173"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, filepath='weights-improvement-10-2.8777.hdf5'):\n",
    "    model.load_weights(filepath)\n",
    "    print(\"Created model and loaded weights from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(n):\n",
    "    \n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    # Pick a random seed\n",
    "    start = np.random.randint(0, len(dataX) - 1)\n",
    "    pattern = dataX[start]\n",
    "    print('Seed:', ''.join([int_to_char[value] for value in pattern]))\n",
    "    \n",
    "    for i in range(n):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(vocab_size)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_char[index]\n",
    "        seq_in = [int_to_char[value] for value in pattern]\n",
    "        sys.stdout.write(result)\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "    print()\n",
    "\n",
    "class PeakOnGeneration(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        generate_text(200)\n",
    "        return\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
